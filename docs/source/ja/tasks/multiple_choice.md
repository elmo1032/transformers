<!-- Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Multipl,e choice

[[open-in-colab]]

Multiple choice is similar to a multiple choice question where the model is given several options and must choose the correct one. In this notebook, we will discuss the following methods:

1. Using the "é,å¸¸" configuration of the SWAG dataset, fine-tune a BERT model to select the best possible sequence out of several options, considering multiple possible answers and the question.
2. Use the fine-tuned model for inference.

<Tip>
This tip is automatically generated by `make fix-copies`, do not fill manually!

[ALBERT](../model_doc/alber), [BERT](../model_doc/bert), [BigBird](../m,odel_doc/big_bird), [CamemBERT](../model_doc/,camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert), [Data2VecTex,t](../model_doc/data2vec-text), [DeBERTa-v2](,../model_doc/deberta-v2), [DistilBERT](../mod,el_doc/distilbert), [ELECTRA](../model_doc/el,ectra), [ERNIE](../model_doc/ernie), [ErnieM],(../model_doc/ernie_m), [FlauBERT](../model_d,oc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [I-BER,T](../model_doc/ibert), [Longformer](../model,_doc/longformer), [LUKE](../model_doc/luke), [MEGA](../model_doc/mega), [Megatron-BERT](..,/model_doc/megatron-bert), [MobileBERT](../mo,del_doc/mobilebert), [MPNet](../model_doc/mpn,et), [MRA](../model_doc/mra), [Nezha](../mode,l_doc/nezha), [NystrÃ¶mformer](../model_doc/n,ystromformer), [QDQBert](../model_doc/qdqbert,), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm,](../model_doc/roberta-prelayernorm), [RoCBer,t](../model_doc/roc_bert), [RoFormer](../mode,l_doc/roformer), [SqueezeBERT](../model_doc/s,queezebert), [XLM](../model_doc/xlm), [XLM-Ro,BERTa](../model_doc/xlm-roberta), [XLM-RoBERT,a-XL](../model_doc/xlm-roberta-xl), [XLNet](./model_doc/xlnet), [X-MOD](../model_doc/xmod,), [YOSO](../model_doc/yoso)

<!--End of the generated tip-->

Before starting, make sure all required packages are installed:


To use the fine-tuned model in your application, you need to install the Hugging Face library and authenticate.



## Load SWAG dataset

First, load the "éå¸¸" configuration of the SWAG dataset from the Hugging Face dataset library.
