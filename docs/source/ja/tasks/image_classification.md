<!-- Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Image classification

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)

<Youtube id="tjAIM7BOYhw"/>

Classifying images involves classifying an image into a category or class. The input to the model is the image, and the output is the predicted class. There are many use cases for image classification, such as identifying objects in an image, image tagging, and image recognition. It can also be used for image quality assessment, image anomaly detection, and more.

In this notebook, we will cover the following methods:

1. Fine-tuning a pre-trained Vision Transformer (ViT) model on the Food-101 dataset.
2. Using the fine-tuned model for image classification.

<Tip>
This tip is automatically generated by `make fix-copies`, do not fill manually!
</Tip>

[BEiT](../model_doc/beit), [BiT](../model_doc/bit), [ConvNeXT](../model_doc/convnext), [ConvNeXTV2](../model_doc/convnextv2), [CvT](../model_doc/cvt), [Data2VecVision](../model_doc/data2vec-vision), [DeiT](../model_doc/deit), [DiNAT](../model_doc/dinat), [DINOv2](../model_doc/dinov2), [EfficientFormer](../model_doc/efficientformer), [EfficientNet](../model_doc/efficientnet), [FocalNet](../model_doc/focalnet), [ImageGPT](../model_doc/imagegpt), [LeViT](../model_doc/levit), [MobileNetV1](../model_doc/mobilenet_v1), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilenetv2), [MobileViTV2](../model_doc/mobilevitv2), [NAT](../model_doc/nat), [Perceiver](../model_doc/perceiver), [PoolFormer](../model_doc/poolformer), [PVT](../model_doc/pvt), [RegNet](../model_doc/regnet), [ResNet](../model_doc/resnet), [SegFormer](../model_doc/segformer), [SwiftFormer](../model_doc/swiftformer), [Swin Transformer](../model_doc/swin), [Swin Transformer V2](../model_doc/swinv2), [VAN](../model_doc/van), [ViT](../model_doc/vit), [ViT Hybrid](../model_doc/vit_hybrid), [ViTMSN](../model_doc/vit_msn)

<!--End of the generated tip-->

Before we begin, make sure all required packages are installed:




Log in to the Hugging Face Hub, download the pre-trained model, and load the dataset:




Split the dataset into train and test sets:




Inspect a sample from the dataset:




Each sample in the dataset has two features:

- `image`: PIL image of the food item
- `label`: ID of the food class

Create a dictionary to map class labels to IDs and vice versa:




Now, you can convert class IDs to class labels:




## Preprocess

Preprocess the images using the pre-trained ViT image processor:




<frameworkcontent>
<pt>

Apply various image transformations to improve the model's performance. We will use `torchvision`'s `transforms` module for this purpose. These transformations include resizing, cropping, normalization, and more.

